{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flaten 28 * 28 images to a 784 vector for each image\n",
    "\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# Scaling the pixel values(0 - 255) to get them in the range of 0 - 1 \n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "#One hot encodeing the output classes\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim = num_pixels, kernel_initializer = 'normal', activation = 'relu'))\n",
    "   \n",
    "    model.add(Dense(num_classes, kernel_initializer = 'normal', activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.2856 - acc: 0.9188 - val_loss: 0.1415 - val_acc: 0.9584\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1119 - acc: 0.9675 - val_loss: 0.1010 - val_acc: 0.9707\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.0720 - acc: 0.9791 - val_loss: 0.0798 - val_acc: 0.9763\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0507 - acc: 0.9850 - val_loss: 0.0757 - val_acc: 0.9768\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0364 - acc: 0.9903 - val_loss: 0.0663 - val_acc: 0.9802\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0269 - acc: 0.9928 - val_loss: 0.0647 - val_acc: 0.9806\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0195 - acc: 0.9956 - val_loss: 0.0649 - val_acc: 0.9806\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0147 - acc: 0.9968 - val_loss: 0.0601 - val_acc: 0.9813\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0113 - acc: 0.9977 - val_loss: 0.0650 - val_acc: 0.9805\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0081 - acc: 0.9985 - val_loss: 0.0601 - val_acc: 0.9819\n",
      "Baseline Error: 1.81%\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "\n",
    "model.fit(X_train,y_train, validation_data = (X_test, y_test), epochs = 10, batch_size = 200, verbose = 2)\n",
    "\n",
    "scores = model.evaluate(X_test,y_test,verbose = 2)\n",
    "\n",
    "print(\"Baseline Error: %.2f%%\" % (100 - scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to implement the same task using a Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test,y_test) = mnist.load_data()\n",
    "\n",
    "# Reshpaing data to [samples, Grayscale =1 / Color = 3, width,height]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],1,28,28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28).astype('float32')\n",
    "\n",
    "#Normalise inputs\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train) \n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "num_class = y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(24,(5,5),input_shape=(1,28,28),activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation = 'relu'))\n",
    "    model.add(Dense(num_class,activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 53s 889us/step - loss: 0.2562 - acc: 0.9262 - val_loss: 0.0781 - val_acc: 0.9768\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 55s 911us/step - loss: 0.0765 - acc: 0.9772 - val_loss: 0.0555 - val_acc: 0.9808\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 55s 915us/step - loss: 0.0552 - acc: 0.9834 - val_loss: 0.0485 - val_acc: 0.9837\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 54s 904us/step - loss: 0.0431 - acc: 0.9866 - val_loss: 0.0387 - val_acc: 0.9874\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 53s 884us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0388 - val_acc: 0.9869\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 53s 881us/step - loss: 0.0282 - acc: 0.9915 - val_loss: 0.0337 - val_acc: 0.9892\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 50s 835us/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0353 - val_acc: 0.9891\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 51s 851us/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0333 - val_acc: 0.9884\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 51s 847us/step - loss: 0.0170 - acc: 0.9946 - val_loss: 0.0326 - val_acc: 0.9892\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 52s 869us/step - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0333 - val_acc: 0.9896\n",
      "CNN error :1.04%\n"
     ]
    }
   ],
   "source": [
    "model = baseline_cnn()\n",
    "\n",
    "model.fit(X_train,y_train, validation_data = (X_test,y_test), epochs = 10, batch_size = 200, verbose = 1)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "print(\"CNN error :%.2f%%\" %(100 - scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
